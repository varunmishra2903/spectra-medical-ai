# M6 â€” Data Preprocessing Contribution Summary
**Author:** Priyotosh (M6 â€” Data Engineer)  
**Last Updated:** 19 December 2025

---

# ğŸ¯ 1. Overview
After completing the dataset validation stage (file counts, folder structure, integrity checks), I moved into the **preprocessing phase** â€” preparing medical imaging datasets for training.

This document explains **what I did**, **what problems I encountered**, and **how I solved them**, in a clear and teamâ€‘friendly way.

---

# ğŸ“¦ 2. Datasets Prepared
### âœ” BraTS 2021 (Brain MRI â€” For M2)
- Downloaded and extracted
- Verified `.nii.gz` integrity

### âœ” RSNA Pneumonia (Chest X-Ray â€” For M3)
- Downloaded and extracted
- Verified DICOM & CSV structure

### âœ” MedMNIST (Gatekeeper â€” For M1)
- Script-based dataset extraction
- Generated sample routing images

### âœ” **MURA v1.1 (Bone X-Ray Fracture Dataset â€” For M4)**
**This is the dataset I fully preprocessed using an advanced pipeline.**

---

# âš™ï¸ 3. What Preprocessing I Performed (Advanced Levelâ€‘2)
This was more than basic histogram equalization. I implemented a **stronger, clinically meaningful preprocessing pipeline**:

### ğŸ”¹ Center Cropping
Removes unwanted black borders and improves region focus.

### ğŸ”¹ Gaussian Blur (Noise Reduction)
Removes X-ray graininess.

### ğŸ”¹ **CLAHE (Contrast Limited Adaptive Histogram Equalization)**
Far more advanced than normal histogram equalization â€” improves local contrast and reveals fractures clearly.

### ğŸ”¹ Z-score Intensity Normalization
Ensures stable training across varying exposure levels.

### ğŸ”¹ Resize to 224Ã—224
Standard input size for common CNN architectures.

### ğŸ”¹ Normalization back to uint8
Ensures PNG safe saving.

### ğŸ”¹ Safe Short Filenames
Prevents Windows MAX_PATH failures.

### ğŸ”¹ Full Metadata CSV Generation
Creates a usable training reference for M4.

---

# ğŸ§ª 4. Problems I Encountered & How I Solved Them
## âŒ Issue 1 â€” Script Only Processed 51 Images
**Cause:** Directory traversal broke due to hidden files and folder structure mismatch.

**Fix:** Added strict directory checks at each folder level.

---

## âŒ Issue 2 â€” Windows Explorer Showed ~37,312 Files Instead of 40,000+
**Cause:** Windows Explorer caching + hidden files not displayed.

**Fix:** Used Python counters to verify actual counts (source of truth).

---

## âŒ Issue 3 â€” About 2,700 Images Didnâ€™t Save
**Cause:** Windows MAX_PATH limitation silently blocked long filenames.

**Fix:** Implemented short, safe filenames like:
```
mura_000001.png
mura_000002.png
```
This solved the silent write failures.

---

## âŒ Issue 4 â€” Some Images Were Corrupted
**Cause:** A few X-ray files could not be read by OpenCV.

**Fix:** Logged them in:
```
failed_images.txt
```
Only **4** images failed â€” normal for large medical datasets.

---

# ğŸ‰ 5. Final Output Summary
### âœ” **40,005 fully processed MURA images**
### âœ” **metadata.csv** generated for training usage
### âœ” **4** corrupted images logged in `failed_images.txt`
### âœ” Dataset is fully consistent, clean, and MLâ€‘ready

Final structure:
```
processed/mura/
    images/               â† 40,005 images
    metadata.csv
    failed_images.txt
```

---

# ğŸš€ 6. Why This Work Matters
The advanced preprocessing pipeline improves:
- Image clarity
- Contrast
- Model stability
- Fracture visibility
- Training accuracy
- Clean reproducibility

This dramatically reduces the workload for M4 and ensures a highâ€‘quality dataset.

---

# ğŸ§  7. What My Teammates Should Know
- The **entire MURA pipeline is complete**: downloading â†’ validating â†’ preprocessing â†’ metadata.
- I implemented **advanced enhancement techniques**, not basic histogram methods.
- M4 can now start training **immediately** with no cleanup required.
- All failures and corrections are **logged and documented**.

---

# ğŸ 8. Next Steps for Me
- Support M3 & M4 with DataLoader integration
- Provide QA visualization tools
- Assist with model verification and debugging
- Maintain structured documentation in `docs/`

---

This document represents my completed contributions after dataset validation, covering full preprocessing, debugging, and delivery of clean ML-ready data.

